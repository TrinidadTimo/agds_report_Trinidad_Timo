---
title: "report_4"
author: "Timo Trinidad"
date: "2025-04-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(readr)
library(lubridate)
library(dplyr)
library(patchwork)
```


# Downloading the Data:
```{r}
url <- "https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/df_for_stepwise_regression.csv"

flux_data <- read.table(
  url,
  header = TRUE,
  sep = ","
)

```

# Model Variable selection:
```{r}
View(flux_data)

sum(!is.na(flux_data$LW_IN_F_MDS))
dim(na.omit(flux_data[,-c(1,2,12)]))


vars <- flux_data[,-c(1,2,12)]
vars <- vars[,c(13,1:12,14)] #rearranging columns

```

- In the course of a first look on the datatable I noticed that the gapfilled lonwave radiation (LW_IN_F_MDS) seems to have quite some missing values. 
As the code in the following line shows, it only contains 11954 non-missing values. As the following regression models demands equal numbers of observations over all included variables, including LW_IN_F_MDS would ineviteably imply losing almost half of all observations over all variables. Based on this concern and drawing on the fact that the incoming longwave radiation is also covered by LW_IN_F, I decide to leave LW_IN_F_MDS out of the vector of the model variables. 

Besides that I'm also not considering siteid (obviously) and TIMESTEP (as we have the most physical variables causally linked to GPP and varying over time included, the effect of a time variable will be very small/insignificant). 

This selection yields a sample of n= 15442 observations (which are ~67% of the initial sample) over 14 variables.

# Exercise 1: 
```{r}
plot_vec <- list()
metrics <- c()

for (i in 2:length(vars)) {
  mod_1 <- lm(vars[,1] ~ vars[,i], na.action = na.omit)
  mod_sum <- summary(mod_1)
  
  metrics["R^2"] <- mod_sum$r.squared
  metrics["RMSE"] <- sqrt(mean(mod_sum$residuals^2))
  metrics["Bias"] <- mean(mod_sum$residuals)
  metrics["Slope"] <- mod_sum$coefficients[2,1]
  
  df_plot <- data.frame(
    Y = vars[as.numeric(rownames(mod_1$model)),1],
    Y_hat = mod_1$fitted.values
  )
  
  plot_vec[[i-1]] <- ggplot(df_plot, aes(x = Y_hat, y = Y)) +
    geom_point(alpha = 0.1) +
    labs(
      x = expression(hat(Y)),
      y = "Y",
      title = colnames(vars)[i],
      subtitle = paste(paste(names(metrics), round(metrics, 3), sep = " = "), collapse = ", ")
    ) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "darkgreen") +
    geom_abline(intercept = mod_sum$coefficients[1,1], slope = mod_sum$coefficients[2,1], color = "red") + 
    coord_cartesian(xlim = c(-5,15), ylim = c(-5,15)) +
    theme_classic() +
    theme(plot.subtitle = element_text(size = 7))
}

```

Visualisation:

```{R}
wrap_plots(plot_vec[11:12], ncol = 2)
```

Analysis: 



# Exercise:
## Model Algorith:

```{r}
mod_pred <- c(colnames(vars)[1])
R_max = 0
AIC_old = -1

  for (j in 2:(ncol(vars))) {
    p = 0
    for (i in 2:ncol(vars)) {
      mod_pred[j] <- colnames(vars)[i]
      if (length(mod_pred) < 3) {
       mod_2 <- lm(as.formula(paste(mod_pred, collapse = "~")), data = vars) 
      } else {
      mod_2 <- lm(as.formula(paste(mod_pred[1], "~",paste(mod_pred[-1], collapse ="+"))), data = vars, na.action =  na.omit) }
      R <- summary(mod_2)$r.squared
      if (R > R_max) {
        R_max <- R
        p <- i
      }
      
      AIC = nrow(vars)* log(sum(mod_2$residuals^2)/nrow(vars)) + 2*(length(mod_pred) + 1) # just "+1 as mod_pred also includes dependent variable.
    }
    mod_pred[j] <- colnames(vars)[p]
    if (AIC > AIC_old & AIC_old >0) {
      mod_pred <- mod_pred[-length(mod_pred)]
      break
    } else {
      AIC_old <- AIC
    }
  } 

summary(mod_2)

```

Höherer AIC-Wert nach Aufrufen von na.omit() direkt in Regressionsformel kommt zustande, da n gestiegen ist. Werden nun zwei zusätzliche Prädikatoren berücksichtigt. Adjusted R^2 steigt leicht. Interessant auch; waren vorher alle Predikatioren signifikant, sind es zwei nun nicht mehr. Was macht man mit diesen? Ist dies Kolinearität? Zeigt, dass es problematisch sein kann, AIC-Wert absolut zu betrachten. Sollte relativ zu einem anderen AIC-Wert betrachtet werden (grösser oder kleiner)

## Result visualization: 
- fitted Values against actual values
- residual plot
- 


Analysis: 

